
\documentclass[french,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[a4paper,margin=0.85in]{geometry} % Réduire les marges
\usepackage[francais]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{cite} % Pour importer une bibliographie

\usepackage{bbm}
\usepackage{mathtools}
%\usepackage{ulem} % Pour barrer du texte
\usepackage{xcolor} % Pour colorer des symboles dans les équations

\usepackage{tikz} % Pour les dessins
\usepackage{enumitem} % Pour mettre des symboles au choix dans les énumérations
\usepackage{xfrac} % Pour faire des petites fractions : \sfrac{5}{7}

\usepackage{algorithm}
\usepackage{algpseudocode} % Pour écrire des algorithmes
\floatname{algorithm}{Algorithme} % ... en français


% Theorems
\theoremstyle{plain}
\newtheorem{theorem}{Théorème}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemme}
\newtheorem{corollary}{Corollaire}

\theoremstyle{definition}
\newtheorem{definition}{Définition}

\theoremstyle{remark}
\newtheorem{remark}{Remarque}
\newtheorem{example}{Exemple}


% Math operators
\newcommand{\scal}[2]{\left\langle #1 , #2 \right\rangle}
\DeclareMathOperator{\IR}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\One}{\mathbbm{1}}
\DeclareMathOperator{\Ccal}{\mathcal{C}}
\DeclareMathOperator{\logsumexp}{logsumexp}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\MAP}{MAP}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\epsilon}{\varepsilon}



\title{Mathematical foundations of data science\\
Blind deconvolution}
\date{January 2018}
\author{Alexis THIBAULT}

\begin{document}

\maketitle

\section{Abstract}
Blur is a common artifact in medical imagery, astronomy, microscopy, or even photography, which is generally undesirable as it obscures significant information.
In many cases the distortion can be modeled as a convolution, with an unknown kernel \cite{fergus2006removing,cho2009fast,cho2010motion,jia2007single,xu2010two}. One must therefore use \emph{blind deconvolution} to estimate the kernel, along with the de-blurred image.
Since there are more unknowns than data, the problem is ill-posed: assumptions are needed on the original image. The typical constraints are expressed as statistics on the distribution of the gradient \cite{joshi2008psf,krishnan2009fast,levi2009using,levin2007blind}, although more complex techniques have also been proposed \cite{miskin2000ensemble,jalobeanu2002satellite}. Here, we study the case of a Gaussian prior on the gradient, and show its limitations. We also consider adding a regularity prior on the kernel.
Blind deconvolution is often performed by alternating estimation of the blur kernel and the corresponding image, within a bayesian framework \cite{richardson1972bayesian,levin2007blind,levin2009understanding,levin2011efficient,krishnan2009fast,joshi2008psf,jia2007single,levi2009using}.
Here we use \emph{maximum a posteriori} ($\MAP$) estimation. We show numerically that, as explained in \cite{levin2011efficient}, maximizing the likelihood of the kernel ($\MAP_k$) before computing the image yields better results than estimating both image and kernel ($\MAP_{x,k}$).




\section{Introduction}
Many photographs taken without a tripod contain disappointing motion blur caused by camera shake. Such degradation is often approximately linear, and except in the special case when complex movement comes into play \cite{shan2007rotational,whyte2012non}, it is shift-invariant. The transformation of the image can thus be modeled as a convolution with a kernel.
In astronomy as well as microscopy, measuring tools often exhibit a wide \emph{point spread function} (PSF), which results in blurry images, in which significant data is obscured. This may be caused by atmospheric aberrations, or simply by diffraction.
In medical imagery \cite{chen2016compressive}, data acquired with compressive sensing often shows blur, which makes readings difficult.

In all these cases, we are interested in recovering an approximation of the original image, while being given only a distorted version of it:
\[
y = k * x + w.
\]
When the \emph{point spread function} (PSF) can be measured, as is sometimes the case in microscopy, the convolution kernel is known.
Restoring the original image is then a \emph{non-blind deconvolution problem}.
This simple inverse problem can be solved efficiently: in the absence of noise, one simply needs to apply the inverse filter in the Fourier domain.

Presence of noise makes things slightly more challenging.
Various image priors may be used \cite{sun2014good}.
A common form of image prior is obtained by conditioning on the derivatives. Let $(D_h x)_i$ and $(D_v x)_i$ denote the horizontal and vertical discrete derivatives of image $x$ at pixel $i$.
Take some model $\rho$ for the repartition of the derivatives, and set
\[
p(x) = \prod_i \rho((D_h x)_i) \rho((D_v x)_i) .
\]
Common settings for function $\rho$ are exponential of power functions \cite{krishnan2009fast,levin2009understanding}, and mixtures of Gaussians (MOG) \cite{levin2011efficient,fergus2006removing}.
In the case of a Gaussian prior, which corresponds to $\rho(v) \propto e^{-\frac{|v|}{2\sigma^2}}$, we obtain Wiener deconvolution, which may be computed very efficiently in the Fourier domain, but results in ringing artifacts \cite{shan2008high}.
More advanced methods have been devised for this problem \cite{sun2014good,schmidt2013discriminative}.



In most real-world cases, however, the kernel is unknown. This makes the problem of \emph{blind deconvolution} even more challenging.
Based on the blurry image, it aims at estimating both kernel and image.
Since the number of unknowns exceeds the data, the problem is ill-posed; it is therefore necessary to introduce assumptions about natural images.
Most commonly, a regularity prior on the image is used \cite{krishnan2009fast,fergus2006removing,levin2007blind,levi2009using,levin2011efficient,levin2009understanding}.
Again, it is often conditioned with the image derivatives, using various models.



In practice, $\MAP_{x,k}$ with a Gaussian prior corresponds to alternately performing Wiener deconvolution, and kernel estimation; and $\MAP_k$ is extremely similar, but with an extra term in the kernel estimation.


\section{Mathematical formulation}
\subsection{Problem}
Suppose we are given a distorted observation $y$ of a ground truth signal $x$, and we would like to recover x. The distortion is a convolution with an unknown kernel $k$, followed by the introduction of noise $w$.

We will focus on the case when $x$ and $y$ are two-dimensional grids of pixels, but the methods can be easily generalized to higher dimensions.


\subsection{Méthode de résolution}
On choisit un modèle pour les images naturelles, donné par une fonction de densité $p_x$, un modèle pour les noyaux $p_k$, et un modèle pour le bruit $p_w$.
Étant donnés ces modèles, on effectue une estimation pour le noyau en considérant le \emph{maximum a posteriori} ($\MAP_k$)~:
\[
\hat{k} = \argmax p\left( k \mid y \right) = \argmax \int p\left( x,k \mid y \right) dx,
\]
puis on calcule la déconvolution (non aveugle) à partir de ce noyau. La probabilité conditionnelle est donnée par les formules classiques~:
\[
p\left(x,k \mid y \right) = \frac{p(x,k,y)}{p(y)} = \frac{p(y \mid x,k) \; p(x) \; p(k)}{p(y)}
\]
Le dénominateur est une constante~; le premier facteur du numérateur est relatif au modèle du bruit $w = y - k*x$, le deuxième au modèle d'images, et le troisième au modèle de noyaux. On peut donc réécrire la fonction à maximiser en faisant apparaître explicitement les modèles~:
\[
\hat{k} = \argmax \int p_x(x) p_k(k) p_w(y - k*x) dx
\]

\subsection{Modèles a priori}

Considérons les opérateurs de dérivation discrète $(f_h,f_v) = ((-1,1),(-1,1)^T)$. Le modèle qu'on prend pour les images naturelles se base sur l'idée que les dérivées sont parcimonieuses. On se donne une fonction $\rho$, mélange de gaussiennes, puis on suppose toutes les coordonnées des dérivées indépendantes de loi $\rho$.
\begin{align*}
\rho((f_h*x)_i) &= \sum_j \frac{\pi_j}{\sigma_j \sqrt{2\pi}}e^{-\frac{1}{2\sigma_j^2}\norm{(f_h*x)_i}^2} \\
p_x(x) &= \prod_i \rho\left((f_h*x)_i \right) \prod_i \rho\left((f_v*x)_i\right) .
\end{align*}
On suppose la loi du noyau uniforme. En prenant un bruit blanc gaussien de variance $\eta^2$ pour $w$, on obtient~:
\[
p(y\mid x,k) = \frac{1}{(\eta\sqrt{2\pi})^N} e^{-\frac{1}{2\eta^2}\norm{k*x-y}^2} .
\]

L'intégrale à maximiser est donc~:
\[
\hat{k} = \argmax \int \exp\left( - \frac{\norm{k*x-y}^2}{2\eta^2} +  \sum_i \log\left(\rho((f_h*x)_i)\rho((f_v*x)_i)\right)  + c \right) .
\]

La difficulté ici est que l'on effectue un calcul d'intégrale non trivial. On s'intéresse donc à des stratégies d'approximation.

\section{Algorithme EM}
Une méthode classique pour calculer un maximum a posteriori (MAP) ou un maximum de vraisemblance (ML, pour \textit{maximum likelihood}) est l'algorithme Espérance-Maximisation (en anglais \textit{expectation-maximization}, ou EM). Cet algorithme permet d'estimer le maximum a posteriori lorsque le modèle probabiliste dépend de variables non observables.

Ici on considère que l'image de départ $x$ est une variable cachée. 



\bibliographystyle{plain}
\bibliography{references}

\end{document}


