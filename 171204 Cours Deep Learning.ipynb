{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "## Parametric models\n",
    "For some \"universal\" function $f$ with a parameter $\\theta$, find $\\theta$ such that $ y = f(x,\\theta) $ for our data $(x_i,y_i)$.\n",
    "\n",
    "Linear model : $f(x,\\theta) = \\langle x, \\theta \\rangle$.\n",
    "\n",
    "Deep network : $\\theta = (\\theta_1, \\ldots , \\theta_K)$\n",
    "$$f(x,\\theta) = \\theta_K(\\rho(\\ldots \\theta_2(\\rho(\\theta_1(x))) \\ldots )) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization problem\n",
    "\n",
    "$$ \\min_\\beta \\xi(\\beta)$$\n",
    "Discret : $\\xi(\\beta) = \\frac{1}{n} \\sum \\xi_i(\\beta) = \\frac{1}{n} \\sum L(f(x_i,\\beta), y_i) $.\n",
    "\n",
    "Continu : $\\xi(\\beta) = \\mathbb{E} [ L(f(X,\\beta),Y ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient\n",
    "$$\\beta_{\\ell+1} = \\beta_\\ell - \\tau_\\ell \\nabla \\xi(\\beta_\\ell)$$\n",
    "Discret : le calcul du gradient est possible.\n",
    "$$ \\nabla \\xi = \\frac{1}{n} \\sum \\nabla \\xi_i(\\beta) $$\n",
    "\n",
    "Cas continu : gradient stochastique.\n",
    "Si trop de données, on se place dans le cas continu : on tire $I$ unif $\\{1\\ldots n\\}$, puis $\\nabla \\xi(\\beta) = \\mathbb{E}(\\nabla \\xi_I (\\beta))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "On tire $i_\\ell$ i.i.d. unif. $\\{1\\ldots n\\}$.\n",
    "$$\\beta_{\\ell+1} \\leftarrow \\beta_\\ell - \\tau_\\ell \\nabla \\xi_{i_l}(\\beta_\\ell) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__**Thm :**__\n",
    "$\\xi$ est $\\mu$-fortement convexe.\n",
    "$$\\tau_\\ell := \\frac{1}{\\mu(\\ell+1)}$$\n",
    "$$|| \\nabla \\xi_i || \\le C$$\n",
    "$$\\mathbb{E}(|\\beta_\\ell - \\beta^* |^2) \\le \\frac{R}{\\ell+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dém :\n",
    "$$ \\mathbb{E}(| \\beta_{\\ell +1} - \\beta^* |^2 ) =  \\mathbb{E}( | \\beta_\\ell - \\tau_\\ell \\nabla\\xi_{i_\\ell} (\\beta_\\ell) - \\beta^* |^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de gradient\n",
    "\n",
    "Si $\\xi(\\beta) = \\xi_L \\circ \\ldots \\circ \\xi_0(\\beta)$ :\n",
    "\n",
    "Passe \"forward\" : on pose $\\beta_0 = \\beta$, puis\n",
    "$$\\beta_{k+1} =\\xi_k(\\beta_k)$$\n",
    "\n",
    "Calcul du gradient :\n",
    "$$\\partial \\xi(\\beta) = \\partial \\xi_L (\\beta_L) \\circ \\ldots \\circ \\partial \\xi_0(\\beta_0)$$\n",
    "On peut aussi l'écrire : (backpropagation)\n",
    "\n",
    "$$ \\nabla \\xi(\\beta) = [\\partial \\xi_0(\\beta_0)]^t [\\partial \\xi_1(\\beta_1)]^t  \\ldots [\\partial \\xi_{L-1}(\\beta_{L-1})]^t \\nabla \\xi_L(\\beta_L)$$\n",
    "(Généralisable à des graphes de calcul arbitraires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
